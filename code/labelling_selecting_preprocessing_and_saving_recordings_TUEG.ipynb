{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa60916d-dbc3-43bf-8169-f4de5233e4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "import glob\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "from braindecode.datasets import TUH\n",
    "from braindecode.preprocessing import (\n",
    "    preprocess, Preprocessor, create_fixed_length_windows, scale as multiply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10cf6554-6d8c-4f0b-87c2-d0c76fe8ea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "TUH_PATH = 'please insert actual path to data here'\n",
    "N_JOBS = 2  # specify the number of jobs for loading and windowing\n",
    "tuh = TUH(\n",
    "    path=TUH_PATH,\n",
    "    recording_ids=None,\n",
    "    target_name=None,\n",
    "    preload=False,\n",
    "    add_physician_reports=False,\n",
    "    n_jobs=N_JOBS,  \n",
    ")\n",
    "\n",
    "\n",
    "tuh. description.to_csv('./tuh_eeg_complete_description.csv')\n",
    "\n",
    "reports = pd.read_csv('./tuh_eeg_complete_description.csv',  index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b49428c9-1a27-424b-8eaf-ea0bee29b2f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "610bf03d-deb8-4308-b203-b8e14ab4f776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(impression) 69582\n",
      "len(interpretation) 12384\n"
     ]
    }
   ],
   "source": [
    "impression = []\n",
    "interpretation_idx = []\n",
    "for irec in range(0,len(reports)): #69582\n",
    "    text = str(reports.report[irec]).lower()\n",
    "    if text.find('impression:') != -1:\n",
    "        start = text.index('impression:') # 'IMPRESSION'\n",
    "        if text[start:].find('clinical')!= -1:\n",
    "            end = text[start:].index('clinical')\n",
    "            endtext = start + end \n",
    "            impression.append(text[start:endtext])\n",
    "        else:\n",
    "            if text[start:].find('.\\n\\n')!= -1:\n",
    "                end = text[start:].index('.\\n\\n')\n",
    "                endtext = start + end \n",
    "                impression.append(text[start:endtext])\n",
    "            else:\n",
    "                end = text[start:].index('\\n')\n",
    "                endtext = start + end \n",
    "                impression.append(text[start:endtext])  \n",
    "    elif text.find('impression/clinical') != -1:\n",
    "        start = text.index('impression/clinical') # 'IMPRESSION'\n",
    "       # if text[start:].find('CLINICAL')!= -1:\n",
    "       #     end = text[start:].index('CLINICAL')\n",
    "      ##      endtext = start + end \n",
    "        #impression.append(text[start:endtext])\n",
    "       # else:\n",
    "       \n",
    "        if text[start:].find('.\\n\\n')!= -1:\n",
    "            end = text[start:].index('.\\n\\n')\n",
    "            endtext = start + end \n",
    "            impression.append(text[start:endtext])\n",
    "        else:\n",
    "            end = text[start:].index('\\n')\n",
    "            endtext = start + end \n",
    "            impression.append(text[start:endtext])     \n",
    "    elif text.find('impression & clinical') != -1:\n",
    "        start = text.index('impression & clinical') # 'IMPRESSION'\n",
    "       # if text[start:].find('CLINICAL')!= -1:\n",
    "       #     end = text[start:].index('CLINICAL')\n",
    "      ##      endtext = start + end \n",
    "        #impression.append(text[start:endtext])\n",
    "       # else:\n",
    "       \n",
    "        if text[start:].find('.\\n\\n')!= -1:\n",
    "            end = text[start:].index('.\\n\\n')\n",
    "            endtext = start + end \n",
    "            impression.append(text[start:endtext])\n",
    "        else:\n",
    "            end = text[start:].index('\\n')\n",
    "            endtext = start + end \n",
    "            impression.append(text[start:endtext])             \n",
    "    elif text.find('impression') != -1:\n",
    "        start = text.index('impression') # 'IMPRESSION'\n",
    "        if text[start:].find('clinical')!= -1:\n",
    "            end = text[start:].index('clinical')\n",
    "            endtext = start + end \n",
    "            impression.append(text[start:endtext])\n",
    "        else:\n",
    "            if text[start:].find('.\\n\\n')!= -1:\n",
    "                end = text[start:].index('.\\n\\n')\n",
    "                endtext = start + end \n",
    "                impression.append(text[start:endtext])\n",
    "            else:\n",
    "                end = text[start:].index('\\n')\n",
    "                endtext = start + end \n",
    "                impression.append(text[start:endtext])           \n",
    "\n",
    "        \n",
    "    elif text.find('interpretation') != -1:\n",
    "        start = text.index('interpretation') # 'IMPRESSION'\n",
    "       # if text[start:].find('CLINICAL')!= -1:\n",
    "       #     end = text[start:].index('CLINICAL')\n",
    "      ##      endtext = start + end \n",
    "        #impression.append(text[start:endtext])\n",
    "        interpretation_idx.extend([irec])\n",
    "       # else:\n",
    "        if text[start:].find('.\\n\\n')!= -1:\n",
    "            end = text[start:].index('.\\n\\n')\n",
    "            endtext = start + end \n",
    "            impression.append(text[start:endtext])\n",
    "        else:\n",
    "            end = text[start:].index('\\n')\n",
    "            endtext = start + end \n",
    "            impression.append(text[start:endtext])  \n",
    "    else: impression.append('nan')      \n",
    "        \n",
    "print('len(impression)',len(impression)) #should be    69582\n",
    "print('len(interpretation)',len(interpretation_idx)) #should be    12384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41271fc6-91f7-41b6-98a3-4e4b3e538dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "impression = [w.replace(':\\n', ' ') for w in impression]\n",
    "\n",
    "impression = [w.replace('.\\n', '. ') for w in impression]\n",
    "impression =[w.replace('\\t', '') for w in impression]\n",
    "\n",
    "\n",
    "\n",
    "reports['impression'] = impression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cdff9d0-7b7f-4304-bfec-b53189f36dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69582\n",
      "69582\n"
     ]
    }
   ],
   "source": [
    "import re  # newlabelling\n",
    "## find all recors with abnormal impression\n",
    "abnormal = []\n",
    "\n",
    "for irec in range(0,len(reports)): #69582\n",
    "    text = str(reports.impression[irec])\n",
    "    text = text.lower()\n",
    "\n",
    "    regex1=re.compile(r\"\\babnormal\")\n",
    "    regex2=re.compile(r\"\\bno normal\")\n",
    "    regex3=re.compile(r\"\\bnot normal\")\n",
    "    regex4=re.compile(r\"\\bno abnormal\")\n",
    "    regex5=re.compile(r\"\\bnot abnormal\")\n",
    "    regex6=re.compile(r\"\\bnormal eeg\")\n",
    "    regex7=re.compile(r\"\\ba normal\")\n",
    "    regex8=re.compile(r\"\\babsence of a normal \")\n",
    "    regex9=re.compile(r\"\\breplacement of a normal \")\n",
    "    regex10=re.compile(r\"\\babsence of normal \")\n",
    "    regex11=re.compile(r\"\\babnormal eeg\")\n",
    "    regex12=re.compile(r\"\\bno epileptiform abnormalities\")  \n",
    "    regex13=re.compile(r\"\\bis just outside of the range of normal\")\n",
    "    if (regex1.findall(text) != [] or regex2.findall(text) != [] or regex3.findall(text) != [] or regex8.findall(text) != [] or regex9.findall(text) != [] or regex10.findall(text) != [] or regex11.findall(text) != [] or regex13.findall(text) != [] ) and\\\n",
    "       (regex4.findall(text) == [] and regex5.findall(text) == []  and regex6.findall(text) == []  and \\\n",
    "         (regex7.findall(text) == [] or (regex7.findall(text) != [] and regex8.findall(text) != []) or (regex7.findall(text) != [] and regex11.findall(text) != []) or \\\n",
    "          (regex7.findall(text) != [] and regex9.findall(text) != []) or (regex7.findall(text) != [] and regex10.findall(text) != []) or\\\n",
    "         (regex7.findall(text) == [] and regex9.findall(text) == []))):\n",
    "        abnormal.append(True)  #\n",
    "         \n",
    "    elif text.lower()== 'nan':\n",
    "        abnormal.append('nan')    \n",
    "    else: abnormal.append(False)    \n",
    "    del regex1, regex2, regex3, regex4,regex5,regex6,regex7, regex8, regex9,regex10,regex11,regex12, regex13,text    \n",
    "print(len(abnormal)) # should be 69582\n",
    "\n",
    "reports['pathological'] = abnormal #69582\n",
    "\n",
    "## find all recors with normal impression\n",
    "normal = []\n",
    "for irec in range(0,len(reports)):\n",
    "\n",
    "    if reports.pathological[irec] == False:\n",
    "        text = str(reports.impression[irec])\n",
    "        text = text.lower()\n",
    "        regex1=re.compile(r\"\\bnormal\")\n",
    "        regex2=re.compile(r\"\\bno abnormal\")\n",
    "        regex3=re.compile(r\"\\bnot abnormal\")\n",
    "        regex4=re.compile(r\"\\bno normal\")\n",
    "        regex5=re.compile(r\"\\bnot normal\")\n",
    "        regex6=re.compile(r\"\\babnormal eeg\")\n",
    "        if (regex1.findall(text) != [] or regex2.findall(text) != [] or regex3.findall(text) != []) and (regex4.findall(text) == [] and regex5.findall(text) == [] and regex6.findall(text) == []):\n",
    "            normal.append(True)\n",
    "        else:\n",
    "            normal.append('no label')\n",
    "        del regex1, regex2, regex3, regex4, regex5,regex6, text    \n",
    "    if reports.pathological[irec] == 'nan':\n",
    "        normal.append('nan')    \n",
    "    if reports.pathological[irec] == True:   \n",
    "        normal.append(False)\n",
    "    #del text    \n",
    "print(len(normal)) # should be 69582\n",
    "\n",
    "reports['normal'] = list(normal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5d0d5e27-5298-47a9-9890-585bfeb55666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num records:  69582\n",
      "abnormal : 38399\n",
      "normal : 13669\n",
      "no keyword found : 14565\n",
      "no Impression or Report : 2949\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None, None, None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occurrences = abnormal.count(True)\n",
    "normal_occ = normal.count(True)\n",
    "nokey = normal.count('no label')\n",
    "nan = abnormal.count('nan')\n",
    "print('Num records: ', len(abnormal)), print('abnormal :', occurrences) , print('normal :', normal_occ), print('no keyword found :', nokey), print('no Impression or Report :', nan)\n",
    "\n",
    "# Num records:  69582\n",
    "# abnormal : 38399\n",
    "# normal : 13669\n",
    "# no keyword found : 14565\n",
    "# no Impression or Report : 2949\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "99c4717c-7bb8-4d7e-a212-4ea080e41595",
   "metadata": {},
   "outputs": [],
   "source": [
    "reports.to_pickle(\"./reports_TUEG_labelled.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5ea3b03a-4646-4fe1-82d4-5afd4af0fe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### selected chans and duration\n",
    "label_all_tuh =  reports \n",
    "#pd.read_pickle(\"./reports_labelled.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5d9104c4-0cb6-4ec1-8ee8-903ab835db51",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact = pd.read_csv(\"./artifact_corpus_labels.csv\") \n",
    "#artifact_set\n",
    "\n",
    "eval_set= label_all_tuh\n",
    "all_index = []\n",
    "for iAB in range(len(artifact)):\n",
    "    index = list(eval_set.loc[(eval_set['subject']== artifact['subject'][iAB])& (eval_set['year']== artifact['year'][iAB]) & (eval_set['month']== artifact['month'][iAB]) & (eval_set['day']== artifact['day'][iAB]) &  (eval_set['session']== artifact['session'][iAB]) ].index)\n",
    "    all_index.extend(index)\n",
    "    del index\n",
    "len(all_index)   \n",
    "eval_wo_arti= eval_set.drop(all_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "79aeaeb5-f6c3-4b8a-a898-5ffb70f9aded",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuab = pd.read_pickle(\"./TUHABnormaldescription.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422490e1-59b5-421a-a33c-b6b9d7336d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TUHAB = tuab# delete all subjects from TUHAB\n",
    "eval2= eval_wo_arti\n",
    "\n",
    "all_index = []\n",
    "for iAB in range(len(TUHAB)):\n",
    "    index = list(eval2.loc[(eval2['subject']== TUHAB['subject'][iAB])].index)\n",
    "    all_index.extend(index)\n",
    "    del index\n",
    "\n",
    "\n",
    "\n",
    "eval_final= eval2.drop(all_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "958c72e2-a178-4999-9e86-301ca3b8c825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38674"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_final= eval_final\n",
    "new_labelsa = report_final[(report_final[\"pathological\"] == True)]\n",
    "new_labelsb = report_final[(report_final[\"pathological\"] == False) & (report_final[\"normal\"] == True) ]\n",
    "\n",
    "new_labels = pd.concat([new_labelsa, new_labelsb])\n",
    "\n",
    "len(new_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c59d21e3-b873-4c6f-9875-8dcc6ca7770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted=new_labels.sort_values(by=['year', 'month', 'day', 'subject', 'session', 'segment'])\n",
    "\n",
    "\n",
    "df_sorted['pathological']= df_sorted['pathological'].astype(int)\n",
    "\n",
    "\n",
    "df_sorted= df_sorted.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "97df3289-89f1-40f6-9c3e-36486ca3ac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted.to_pickle('./TUEG_reports_labelled_and_selected.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83628c07-c273-4116-8640-8af7793bd4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## select data based on duration and channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06a35145-d311-4ef0-8a50-6a24d02e661d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import mne\n",
    "mne.set_log_level('ERROR')\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from IPython.utils import io\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.nn.functional import relu\n",
    "\n",
    "from braindecode import EEGClassifier\n",
    "from braindecode.training.losses import CroppedLoss\n",
    "from braindecode.models import Deep4Net,ShallowFBCSPNet\n",
    "from braindecode.util import set_random_seeds\n",
    "from braindecode.models.util import to_dense_prediction_model, get_output_shape\n",
    "\n",
    "from braindecode.datautil.windowers import create_fixed_length_windows\n",
    "from braindecode.datautil.serialization import  load_concat_dataset\n",
    "\n",
    "from braindecode.datasets import BaseConcatDataset\n",
    "\n",
    "\n",
    "from functools import partial \n",
    "from skorch.callbacks import LRScheduler, EarlyStopping,Checkpoint, EpochScoring\n",
    "from skorch.helper import predefined_split\n",
    "\n",
    "\n",
    "    \n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/home/kiessnek/TUHEEG_decoding/')\n",
    "from ImbalancedDatasetSampler import ImbalancedDatasetSampler\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "import GPUtil\n",
    "\n",
    "\n",
    "#######################\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "from braindecode.datasets.tuh import TUHAbnormal, TUH\n",
    "from braindecode.datasets.base import BaseDataset,BaseConcatDataset    \n",
    "from IPython.utils import io\n",
    "\n",
    "\n",
    "import mne\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import mne\n",
    "\n",
    "def get_TUH_labeled(path_TUH, label_file, recording_ids=None):\n",
    "\n",
    "        #from braindecode.datasets.tuh import TUH\n",
    "       # add_physician_reports = True\n",
    "        #self.label_file= label_file\n",
    "        wanted_reports, paths = _load_labels(label_file) #,label_abnormal, label_normal\n",
    "\n",
    "        all_datasets = []\n",
    "        label_list =[]\n",
    "\n",
    "        if recording_ids is None:\n",
    "            recording_ids = len(paths)\n",
    "           \n",
    "        recording_reports = wanted_reports[:recording_ids]\n",
    "        \n",
    "        label_list = recording_reports['pathological'] #['PATHOLOGICAL']\n",
    "        \n",
    "       # recording_reports\n",
    "\n",
    "        \n",
    "        TUH_all_datasets = _load_TUH_labeled(reports_to_load=recording_reports, target_name=None, preload=False,\n",
    "                         add_physician_reports=True)\n",
    "        \n",
    "             \n",
    "        # adapt dataframe \n",
    "        # add dataset and pathologcial column so df. columns are identical between TUHEEG and TUHAbnormal\n",
    "\n",
    "        all_datasets=BaseConcatDataset(TUH_all_datasets)\n",
    "        target = label_list\n",
    "        \n",
    "\n",
    "             \n",
    "        for d, y in zip(all_datasets.datasets, target):\n",
    "            d.description['pathological'] = y\n",
    "            d.target_name = 'pathological'\n",
    "            d.target = d.description[d.target_name]\n",
    "        all_datasets.set_description(pd.DataFrame([d.description for d in all_datasets.datasets]), overwrite=True)\n",
    "\n",
    "       \n",
    "        ds_name = ['TUHEEG'] * len(all_datasets.datasets)\n",
    "        for ds, ys in zip(all_datasets.datasets, ds_name):\n",
    "            ds.description['dataset'] = ys\n",
    "            ds.target_name = 'dataset'\n",
    "            ds.target = ds.description[d.target_name]\n",
    "        all_datasets.set_description(pd.DataFrame([d.description for d in all_datasets.datasets]), overwrite=True)\n",
    "        #all_datasets.set_description = (pd.DataFrame([d.description for d in all_datasets.datasets]),overwrite=True)\n",
    "\n",
    "        all_datasets.description['dataset'] = 'TUHEEG'\n",
    "        \n",
    "\n",
    "        \n",
    "        return all_datasets, label_list\n",
    "        print(len(all_datasets), len(label_list))    \n",
    "\n",
    "def _load_TUH_labeled(reports_to_load, target_name=None, preload=False,\n",
    "                     add_physician_reports=True):\n",
    "    base_datasets = []\n",
    "    #descriptions  = reports_to_load\n",
    "    for file_path_i, file_path in reports_to_load['path'].iteritems():\n",
    "        # parse age and gender information from EDF header\n",
    "        age,  gender = _parse_age_and_gender_from_edf_header(file_path) #test_report['age'][file_path_i]#\n",
    "        #gender = test_report['gender'][file_path_i]\n",
    "        raw = mne.io.read_raw_edf(file_path, preload=preload)\n",
    "        # read info relevant for preprocessing from raw without loading it\n",
    "        sfreq = raw.info['sfreq']\n",
    "        n_samples = raw.n_times\n",
    "        d = {\n",
    "            'sfreq': float(sfreq),\n",
    "            'n_samples': int(n_samples),\n",
    "            'age': int(age),\n",
    "            'gender': gender,\n",
    "            }\n",
    "        if add_physician_reports:\n",
    "            physician_report = _read_physician_report(file_path)\n",
    "            d['report'] = physician_report\n",
    "        additional_description = pd.Series(d)\n",
    "        #description = pd.concat(\n",
    "        #   [reports_to_load, additional_description]) #descriptions.pop(file_path_i)\n",
    "        description =reports_to_load.iloc[file_path_i]\n",
    "        base_dataset = BaseDataset(raw, description,\n",
    "                                   target_name=target_name)\n",
    "        base_datasets.append(base_dataset)\n",
    "\n",
    "\n",
    "    return base_datasets   \n",
    "            \n",
    "def _load_labels(filepath):\n",
    "\n",
    "\n",
    "    reports = pd.read_pickle(filepath)\n",
    "    \n",
    "    wanted_reports = reports#\n",
    "    paths = wanted_reports['path'].reset_index(drop=True)\n",
    "    \n",
    "    wanted_reports = wanted_reports.reset_index(drop=True)\n",
    "\n",
    "    return wanted_reports, paths\n",
    "\n",
    "def _parse_age_and_gender_from_edf_header(file_path):\n",
    "    header = _read_edf_header(file_path)\n",
    "    # bytes 8 to 88 contain ascii local patient identification\n",
    "    # see https://www.teuniz.net/edfbrowser/edf%20format%20description.html\n",
    "    patient_id = header[8:].decode(\"ascii\")\n",
    "    age = -1\n",
    "    found_age = re.findall(r\"Age:(\\d+)\", patient_id)\n",
    "    if len(found_age) == 1:\n",
    "        age = int(found_age[0])\n",
    "    gender = \"X\"\n",
    "    found_gender = re.findall(r\"\\s([F|M])\\s\", patient_id)\n",
    "    if len(found_gender) == 1:\n",
    "        gender = found_gender[0]\n",
    "    return age, gender    \n",
    "\n",
    "def _read_edf_header(file_path):\n",
    "    f = open(file_path, \"rb\")\n",
    "    header = f.read(88)\n",
    "    f.close()\n",
    "    return header\n",
    "\n",
    "def _read_physician_report(file_path):\n",
    "    directory = os.path.dirname(file_path)\n",
    "    txt_file = glob.glob(os.path.join(directory, '**/*.txt'), recursive=True)\n",
    "    # check that there is at most one txt file in the same directory\n",
    "    assert len(txt_file) in [0, 1]\n",
    "    report = ''\n",
    "    if txt_file:\n",
    "        txt_file = txt_file[0]\n",
    "        # somewhere in the corpus, encoding apparently changed\n",
    "        # first try to read as utf-8, if it does not work use latin-1\n",
    "        try:\n",
    "            with open(txt_file, 'r', encoding='utf-8') as f:\n",
    "                report = f.read()\n",
    "        except UnicodeDecodeError:\n",
    "            with open(txt_file, 'r', encoding='latin-1') as f:\n",
    "                report = f.read()\n",
    "    return report\n",
    "\n",
    "    \n",
    "\n",
    "from tqdm.autonotebook import tqdm        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7542bcf-1a2b-4d42-a520-8b51a79b4d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data \n",
      "1233.7344496250153\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "#from combineTUHdatasets import *\n",
    "print('load data ' )\n",
    "start_load = time.time()\n",
    "recording_ids=None\n",
    "with io.capture_output() as captured:\n",
    "    tuh, TUH_label_list = get_TUH_labeled(path_TUH='/data/datasets/TUH/EEG/tuh_eeg/', \n",
    "                                                                     label_file= './TUEG_reports_labelled_and_selected.pkl', \n",
    "                                                                     recording_ids=None) #CombineTUHDatasets.\n",
    "end_load= time.time()\n",
    "print(end_load -start_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c62b5a6f-e9a5-4d00-9345-5ca01e8fcff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8bd731cd-324d-4198-8d72-faa786ad77d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "import mne\n",
    "\n",
    "from braindecode.datasets import TUH\n",
    "from braindecode.datautil.preprocess import preprocess, Preprocessor\n",
    "from braindecode.datautil.windowers import create_fixed_length_windows\n",
    "from braindecode.datautil.serialization import (\n",
    "    save_concat_dataset, load_concat_dataset)\n",
    "\n",
    "mne.set_log_level('ERROR')  # avoid messages everytime a window is extracted\n",
    "\n",
    "\n",
    "def select_by_duration(ds, tmin=0, tmax=None):\n",
    "    # determine length of the recordings and select based on tmin and tmax\n",
    "    duration = ds.description.n_samples / ds.description.sfreq\n",
    "    duration = duration[duration >= tmin]\n",
    "    if tmax is None:\n",
    "        tmax = np.inf\n",
    "    duration = duration[duration <= tmax]\n",
    "    split_ids = list(duration.index)\n",
    "    splits = ds.split(split_ids)\n",
    "    split = splits['0']\n",
    "    return split\n",
    "\n",
    "\n",
    "tmin = 15 * 60\n",
    "tmax = None #n_max_minutes\n",
    "tuh = select_by_duration(tuh, tmin, tmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5798e2e3-ba01-41db-bb77-9a565b1260e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_ch_names = sorted([\n",
    "    'A1', 'A2',\n",
    "    'FP1', 'FP2', 'F3', 'F4', 'C3', 'C4', 'P3', 'P4', 'O1', 'O2',\n",
    "    'F7', 'F8', 'T3', 'T4', 'T5', 'T6', 'FZ', 'CZ', 'PZ'])\n",
    "ar_ch_names = sorted([\n",
    "    'EEG A1-REF', 'EEG A2-REF',\n",
    "    'EEG FP1-REF', 'EEG FP2-REF', 'EEG F3-REF', 'EEG F4-REF', 'EEG C3-REF',\n",
    "    'EEG C4-REF', 'EEG P3-REF', 'EEG P4-REF', 'EEG O1-REF', 'EEG O2-REF',\n",
    "    'EEG F7-REF', 'EEG F8-REF', 'EEG T3-REF', 'EEG T4-REF', 'EEG T5-REF',\n",
    "    'EEG T6-REF', 'EEG FZ-REF', 'EEG CZ-REF', 'EEG PZ-REF'])\n",
    "le_ch_names = sorted([\n",
    "    'EEG A1-LE', 'EEG A2-LE',\n",
    "    'EEG FP1-LE', 'EEG FP2-LE', 'EEG F3-LE', 'EEG F4-LE', 'EEG C3-LE',\n",
    "    'EEG C4-LE', 'EEG P3-LE', 'EEG P4-LE', 'EEG O1-LE', 'EEG O2-LE',\n",
    "    'EEG F7-LE', 'EEG F8-LE', 'EEG T3-LE', 'EEG T4-LE', 'EEG T5-LE',\n",
    "    'EEG T6-LE', 'EEG FZ-LE', 'EEG CZ-LE', 'EEG PZ-LE'])\n",
    "assert len(short_ch_names) == len(ar_ch_names) == len(le_ch_names)\n",
    "ar_ch_mapping = {ch_name: short_ch_name for ch_name, short_ch_name in zip(\n",
    "    ar_ch_names, short_ch_names)}\n",
    "le_ch_mapping = {ch_name: short_ch_name for ch_name, short_ch_name in zip(\n",
    "    le_ch_names, short_ch_names)}\n",
    "ch_mapping = {'ar': ar_ch_mapping, 'le': le_ch_mapping}\n",
    "\n",
    "\n",
    "def select_by_channels(ds, ch_mapping):\n",
    "    split_ids = []\n",
    "    for i, d in enumerate(ds.datasets):\n",
    "        seta = set(ch_mapping[d.description.reference].keys())\n",
    "        setb = set(d.raw.ch_names)\n",
    "        if seta.issubset(setb):\n",
    "            split_ids.append(i)\n",
    "    return ds.split(split_ids)['0']\n",
    "\n",
    "\n",
    "tuh = select_by_channels(tuh, ch_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f0c07c97-0df4-4d50-a6f7-0427fc3a5a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuh.description.to_pickle(\"./selected_labelled_reports_10min.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "eae9ea6c-1602-4ac8-89bc-057def055f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20320, 6820)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tuh.description[tuh.description['pathological']==1]),len(tuh.description[tuh.description['pathological']==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f82349b7-1baf-4aef-adbd-d6bb8d60fa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuh_15 = tuh.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fa0757-1690-435d-a0f7-e1d12f158db3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d18c2e42-dab0-43ec-9360-7ca3d49e36e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuh.description.to_pickle(\"./selected_labelled_reports_15min.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e489b1be-9dc2-44f5-a9d2-61a6f0987f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuh_selected= pd.read_pickle(\"./selected_labelled_reports_15min.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b4d052c-0968-46bf-933d-2371e471476e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10838, 4462, 15300)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tuh_selected[tuh_selected['pathological']==1]),len(tuh_selected[tuh_selected['pathological']==0]), len(tuh_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e3add4dc-2d2d-4d86-9f1a-82ac48eda22a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b30e87d9-0784-4d9a-aea1-d1d64bbc903f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted= tuh_selected.sort_values(by=['year', 'month', 'day', 'subject', 'session', 'segment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8edd5cd2-f4a2-4d62-957f-5247dc194728",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted2=df_sorted.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d02f67-eb13-4a64-9404-979ee2b7aeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spilt in eval and train set SKF 10fold last fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35688e3b-4e30-4dbc-8823-4baa5db587e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold \n",
    "skf = StratifiedKFold(n_splits=10,shuffle=False) \n",
    "target=df_sorted2.loc[:,'pathological']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56ef45a3-ca75-4a73-b5fc-0d9dde47aed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Class Ratio: 0.7084967320261438\n",
      "Fold 2 Class Ratio: 0.7084967320261438\n",
      "Fold 3 Class Ratio: 0.7084967320261438\n",
      "Fold 4 Class Ratio: 0.7084967320261438\n",
      "Fold 5 Class Ratio: 0.7084967320261438\n",
      "Fold 6 Class Ratio: 0.7084967320261438\n",
      "Fold 7 Class Ratio: 0.7084967320261438\n",
      "Fold 8 Class Ratio: 0.7084967320261438\n",
      "Fold 9 Class Ratio: 0.707843137254902\n",
      "Fold 10 Class Ratio: 0.707843137254902\n"
     ]
    }
   ],
   "source": [
    "df=df_sorted2\n",
    "fold_no = 1\n",
    "for train_index, test_index in skf.split(df, target):\n",
    "    train = df.loc[train_index,:]\n",
    "    test = df.loc[test_index,:]\n",
    "    print('Fold',str(fold_no),'Class Ratio:',sum(test['pathological'])/len(test['pathological']))\n",
    "    fold_no += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2370e7-3d75-4f04-853e-ca90667c02ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['train']= True\n",
    "train\n",
    "#13770 rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e86f23-5745-4014-a39b-970ae42286f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['train']= False\n",
    "test\n",
    "\n",
    "#2906"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19d7c716-0468-4be2-9724-5de62a0e13cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.concat([train, test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "347ab347-7204-4b85-ad32-6058e4b54a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train= train.reset_index(drop=True)\n",
    "final_eval=test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be9c74d3-8bd9-4a98-a890-c705d193e980",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_pickle('./final_tuabex_description.pkl')\n",
    "final_train.to_pickle('./final_train_tuabex_description.pkl')\n",
    "final_eval.to_pickle('./final_eval_tuabex_description.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "700a73f4-12fb-4d0d-b3a7-07513a0b5ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_pickle('./final_tuabex_description.pkl')\n",
    "final_train.to_pickle('./final_train_tuabex_description.pkl')\n",
    "final_eval.to_pickle('./final_eval_tuabex_description.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744b4f20-3aa2-4479-a729-41beb88b1adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### preprocess and save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3061f88f-5e3b-40f0-b468-5fa9b849f21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data \n",
      "1284.9824883937836\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print('load data ' )\n",
    "start_load = time.time()\n",
    "recording_ids=None\n",
    "with io.capture_output() as captured:\n",
    "    dataset_train, TUH_label_list = get_TUH_labeled(path_TUH='/data/datasets/TUH/EEG/tuh_eeg/', \n",
    "                                                                     label_file= './final_train_tuabex_description.pkl', \n",
    "                                                                     recording_ids=None) \n",
    "end_load= time.time()\n",
    "print(end_load -start_load)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f8473ee-130e-48f6-a2be-78b52919a311",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted= dataset_train.description.sort_values(by=['year', 'month', 'day', 'subject','session','segment'])\n",
    "ids_sorted = list(df_sorted.index)\n",
    "train_complete = dataset_train.split(ids_sorted)['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e96f10dd-57ee-43ac-b56d-63d643026c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d469520a5747ebab1ac010669a4a3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=13770.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tuh = dataset_train\n",
    "tuh_splits = tuh.split([[i] for i in range(len(dataset_train.datasets))])\n",
    "\n",
    "\n",
    "duration = []\n",
    "\n",
    "\n",
    "for ds, tuh_subset in tqdm(tuh_splits.items()):\n",
    "    duration.append (tuh_subset.description.n_samples / tuh_subset.description.sfreq)\n",
    "\n",
    "duration_train = train_complete.description\n",
    "duration_train['duration']=list(duration)\n",
    "\n",
    "duration_train.to_pickle('./duration_train_tuabextended.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d38b6e71-b28f-496a-bc05-45cce2fe6db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "import mne\n",
    "\n",
    "from braindecode.datasets import TUH\n",
    "from braindecode.datautil.preprocess import preprocess, Preprocessor\n",
    "from braindecode.datautil.windowers import create_fixed_length_windows\n",
    "from braindecode.preprocessing import (\n",
    "    preprocess, Preprocessor, create_fixed_length_windows, scale as multiply)\n",
    "\n",
    "mne.set_log_level('ERROR')  # avoid messages everytime a window is extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d3d8a84-1d73-44bf-83fc-0684e36b0a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_ch_names = sorted([\n",
    "                'A1', 'A2', 'C3', 'C4', 'Cz', 'F3', 'F4', 'F7', 'F8',\n",
    "                'Fp1', 'Fp2', 'Fz', 'O1', 'O2', 'P3', 'P4', 'Pz', 'T3',\n",
    "                 'T4', 'T5', 'T6'\n",
    "            ])\n",
    "ar_ch_names = sorted([\n",
    "    'EEG A1-REF', 'EEG A2-REF',\n",
    "    'EEG FP1-REF', 'EEG FP2-REF', 'EEG F3-REF', 'EEG F4-REF', 'EEG C3-REF',\n",
    "    'EEG C4-REF', 'EEG P3-REF', 'EEG P4-REF', 'EEG O1-REF', 'EEG O2-REF',\n",
    "    'EEG F7-REF', 'EEG F8-REF', 'EEG T3-REF', 'EEG T4-REF', 'EEG T5-REF',\n",
    "    'EEG T6-REF', 'EEG FZ-REF', 'EEG CZ-REF', 'EEG PZ-REF'])\n",
    "le_ch_names = sorted([\n",
    "    'EEG A1-LE', 'EEG A2-LE',\n",
    "    'EEG FP1-LE', 'EEG FP2-LE', 'EEG F3-LE', 'EEG F4-LE', 'EEG C3-LE',\n",
    "    'EEG C4-LE', 'EEG P3-LE', 'EEG P4-LE', 'EEG O1-LE', 'EEG O2-LE',\n",
    "    'EEG F7-LE', 'EEG F8-LE', 'EEG T3-LE', 'EEG T4-LE', 'EEG T5-LE',\n",
    "    'EEG T6-LE', 'EEG FZ-LE', 'EEG CZ-LE', 'EEG PZ-LE'])\n",
    "assert len(short_ch_names) == len(ar_ch_names) == len(le_ch_names)\n",
    "ar_ch_mapping = {ch_name: short_ch_name for ch_name, short_ch_name in zip(\n",
    "    ar_ch_names, short_ch_names)}\n",
    "le_ch_mapping = {ch_name: short_ch_name for ch_name, short_ch_name in zip(\n",
    "    le_ch_names, short_ch_names)}\n",
    "ch_mapping = {'ar': ar_ch_mapping, 'le': le_ch_mapping}\n",
    "\n",
    "\n",
    "\n",
    "def custom_rename_channels(raw, mapping):\n",
    "    # rename channels which are dependent on referencing:\n",
    "    # le: EEG 01-LE, ar: EEG 01-REF\n",
    "    # mne fails if the mapping contains channels as keys that are not present\n",
    "    # in the raw\n",
    "    reference = raw.ch_names[0].split('-')[-1].lower()\n",
    "    assert reference in ['le', 'ref'], 'unexpected referencing'\n",
    "    reference = 'le' if reference == 'le' else 'ar'\n",
    "    raw.rename_channels(mapping[reference])\n",
    "\n",
    "\n",
    "def custom_crop(raw, tmin=0.0, tmax=None, include_tmax=True):\n",
    "    # crop recordings to tmin – tmax. can be incomplete if recording\n",
    "    # has lower duration than tmax\n",
    "    # by default mne fails if tmax is bigger than duration\n",
    "    tmax = min((raw.n_times - 1) / raw.info['sfreq'], tmax)\n",
    "    raw.crop(tmin=tmin, tmax=tmax, include_tmax=include_tmax)\n",
    "\n",
    "\n",
    "n_max_minutes=21\n",
    "tmin = 1 * 60\n",
    "tmax = n_max_minutes * 60\n",
    "sfreq = 100\n",
    "\n",
    "preprocessors = [\n",
    "    Preprocessor(custom_crop, tmin=tmin, tmax=tmax, include_tmax=False,\n",
    "                 apply_on_array=False),\n",
    "\n",
    "    Preprocessor(custom_rename_channels, mapping=ch_mapping,\n",
    "                 apply_on_array=False),\n",
    "    Preprocessor('pick_channels', ch_names=short_ch_names, ordered=True),\n",
    " \n",
    "    Preprocessor(multiply, factor=1e6, apply_on_array=True),\n",
    "    Preprocessor(np.clip, a_min=-800, a_max=800, apply_on_array=True),\n",
    "    \n",
    "    Preprocessor('set_eeg_reference', ref_channels='average', ch_type='eeg'),\n",
    "\n",
    "    Preprocessor('resample', sfreq=sfreq),\n",
    "    Preprocessor('set_meas_date', meas_date=None)\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a43cc48-f904-4b52-9c2f-27fb2cfab982",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train_complete.description['pathological']\n",
    "\n",
    "\n",
    "\n",
    "for d, y in zip(train_complete.datasets, target):\n",
    "    d.description['pathological'] = y\n",
    "    d.target_name = 'pathological'\n",
    "    d.target = d.description[d.target_name]\n",
    "train_complete.set_description(pd.DataFrame([d.description for d in train_complete.datasets]), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013df43b-5ab1-4a99-b0c4-d91ce9413d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(\n",
    "            concat_ds=train_complete,\n",
    "            preprocessors=preprocessors,\n",
    "            n_jobs=4, \n",
    "            save_dir='/home/data/preprocessed_TUABExtended/final_train/', \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e70c19-3e29-4555-8577-3536bc1d4a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_complete.description.to_pickle('./TUABEX_trainset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a2c6234-ac8a-409a-8040-75be8ae61137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data \n",
      "151.72280645370483\n"
     ]
    }
   ],
   "source": [
    "print('load data ' )\n",
    "start_load = time.time()\n",
    "recording_ids=None\n",
    "with io.capture_output() as captured:\n",
    "    dataset_eval, TUH_label_list = get_TUH_labeled(path_TUH='/data/datasets/TUH/EEG/tuh_eeg/', \n",
    "                                                                     label_file= './final_eval_tuabex_description.pkl', \n",
    "                                                                     recording_ids=None) #CombineTUHDatasets.\n",
    "end_load= time.time()\n",
    "print(end_load -start_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c37a4ae-f6f3-4425-9cec-88e1a85e0204",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_complete=dataset_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae64affe-7f09-4dae-9426-82effb39745a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted= dataset_eval.description.sort_values(by=['year', 'month', 'day', 'subject','session','segment'])\n",
    "ids_sorted = list(df_sorted.index)\n",
    "eval_complete = dataset_eval.split(ids_sorted)['0']\n",
    "\n",
    "\n",
    "target = eval_complete.description['pathological']\n",
    "\n",
    "\n",
    "\n",
    "for d, y in zip(eval_complete.datasets, target):\n",
    "    d.description['pathological'] = y\n",
    "    d.target_name = 'pathological'\n",
    "    d.target = d.description[d.target_name]\n",
    "eval_complete.set_description(pd.DataFrame([d.description for d in eval_complete.datasets]), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55493024-ff98-4161-bd86-d8cf1d9ee381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e25798ef50ec439ba4ffcc0334c6585e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1530.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tuh = eval_complete\n",
    "tuh_splits = tuh.split([[i] for i in range(len(eval_complete.datasets))])\n",
    "#tuh_splits2 =  dict(list(tuh_splits.items())[23000:])\n",
    "\n",
    "duration = []\n",
    "\n",
    "\n",
    "for ds, tuh_subset in tqdm(tuh_splits.items()):\n",
    "    duration.append (tuh_subset.description.n_samples / tuh_subset.description.sfreq)\n",
    "\n",
    "duration_eval = eval_complete.description\n",
    "duration_eval['duration']=list(duration)\n",
    "\n",
    "duration_eval.to_pickle('./duration_eval_tuabextended.pkl')\n",
    "#del dataset_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62a6449-87bc-4a5b-96e3-083483d8580c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a440429-e76d-4e1c-9f57-1679bcf931df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<braindecode.datasets.base.BaseConcatDataset at 0x7ff3fd1a0430>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(\n",
    "            concat_ds=eval_complete,\n",
    "            preprocessors=preprocessors,\n",
    "            n_jobs=4,  \n",
    "            save_dir='/home/data/preprocessed_TUABExtended/final_eval/', \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "365e2d53-bb59-468e-9536-a67c416b4337",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_complete.description.to_pickle('./TUABEX_evalset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3a4dc05-e7b4-4e7a-9369-b79443988e7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13770, 1530)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_train), len(final_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4987a2de-c7e6-446a-9e1f-6fa460da4800",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_eval = eval_complete.description\n",
    "final_train=train_complete.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b603a28d-e133-41d1-a017-f41c01f93754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1083, 447)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_eval[final_eval['pathological']==1]), len(final_eval[final_eval['pathological']==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0d879e1-9dbf-45ba-9f5f-e251ba2acb46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9755, 4015)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_train[final_train['pathological']==1]), len(final_train[final_train['pathological']==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a43a7002-e138-4f4f-907c-9abc778d68c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final.to_pickle('./final_tuabex_description.pkl')\n",
    "final_train=pd.read_pickle('./final_train_tuabex_description.pkl')\n",
    "final_eval=pd.read_pickle('./final_eval_tuabex_description.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07940968-1e13-4c61-a0eb-ac93e6c414cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train=pd.read_pickle('./TUABEX_trainset.pkl')\n",
    "final_eval=pd.read_pickle('./TUABEX_evalset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42027bdb-9f34-435c-9e1c-63833f7e4386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9755, 4015)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_train[final_train['pathological']==1]),len(final_train[final_train['pathological']==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b2815dd-8c46-4081-8fcf-91f89f2488f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1083, 447)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_eval[final_eval['pathological']==1]),len(final_eval[final_eval['pathological']==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c8aa3ff-58dc-4f38-bad7-a9e55b35c964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13770, 1530)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_train), len(final_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04a5562c-5d40-4a4a-bb76-d135cd38681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set= final_train\n",
    "eval_set=final_eval\n",
    "\n",
    "train_set['index']=train_set.index\n",
    "\n",
    "\n",
    "eval_set['index']=eval_set.index\n",
    "\n",
    "train_set_normal = train_set[train_set['pathological']==0]\n",
    "train_set_patho = train_set[train_set['pathological']==1].reset_index(drop=True)\n",
    "\n",
    "eval_set_normal = eval_set[eval_set['pathological']==0]\n",
    "eval_set_patho = eval_set[eval_set['pathological']==1].reset_index(drop=True)\n",
    "\n",
    "train_normal = train_set[(train_set['pathological'] == 0)]\n",
    "train_patho = train_set[(train_set['pathological'] == 1)]\n",
    "train_patho = train_patho.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95efc410-c87f-4ca7-ac45-9bcb1ce72cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./indices/indices_TUABEX_patho_train_set.pkl\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(list(train_set_patho['index']), fp) \n",
    "    \n",
    "with open(\"./indices/indices_TUABEX_patho_eval_set.pkl\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(list(eval_set_patho['index']), fp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25b64d16-99f6-4fae-b0c9-44f429bf1130",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./indices/indices_TUABEX_normal_train_set.pkl\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(list(train_set_normal['index']), fp) \n",
    "    \n",
    "with open(\"./indices/indices_TUABEX_normal_eval_set.pkl\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(list(eval_set_normal['index']), fp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a747895-5e4e-4060-8ee9-7eda8a7e3518",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.arange(len(train_patho))\n",
    "\n",
    "r_eval =np.arange(len(eval_set_patho))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a2b36b3-0fad-4895-bd0b-2e16e722795e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pidx_eval =r_eval[0:len(eval_set_patho):3]\n",
    "pidx_train= r[0:len(train_patho):3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22ac94c1-c617-4af7-8c48-9c0c800b7a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(361, 3252)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pidx_eval), len(pidx_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04b6c6d6-7f6b-45b9-a253-9ff0af49c02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "723"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1= list(train_patho.index)\n",
    "list2=list(pidx_train)\n",
    "rest_train_patho= [item for item in list1 if item not in list2]\n",
    "\n",
    "\n",
    "\n",
    "len(rest_train_patho)\n",
    "\n",
    "r = np.arange(len(train_patho))\n",
    "pidx_train2= r[0:len(rest_train_patho):9]\n",
    "\n",
    "len(pidx_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f604c767-75a4-42a0-ab37-2c6e7214303f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1= list(eval_set_patho.index)\n",
    "list2=list(pidx_eval)\n",
    "rest_eval_patho= [item for item in list1 if item not in list2]\n",
    "\n",
    "\n",
    "\n",
    "len(rest_eval_patho)\n",
    "\n",
    "r = np.arange(len(eval_set_patho))\n",
    "pidx_eval2= r[0:len(rest_eval_patho):9]\n",
    "\n",
    "len(pidx_eval2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "84082b62-fbb2-43a8-9d26-658874bc02a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#jfirst every third, then every 9th of the remaining recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0224057e-ea0f-4846-a9b7-8c638d690f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_patho_bal= train_patho.loc[sorted(list(pidx_train)+list(np.array(rest_train_patho)[pidx_train2]))]\n",
    "\n",
    "eval_patho_bal= eval_set_patho.loc[sorted(list(pidx_eval)+list(np.array(rest_eval_patho)[pidx_eval2]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b76d261d-1bf2-4664-b360-97de7182c761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442, 447)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_patho_bal),len(eval_set_normal)  # jedes 9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a647b75b-a78c-4248-b927-08cb911710e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3975, 4015)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_patho_bal),len(train_set_normal)  # jedes 9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "057ac61f-0447-46aa-818f-b99a48dc7d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4015, 3975)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_bal_train = list(sorted(list(train_patho_bal['index'])+ list(train_set_normal['index'])))\n",
    "\n",
    "trainbal = final_train.iloc[ind_bal_train]\n",
    "\n",
    "len(trainbal[trainbal['pathological']==0]),len(trainbal[trainbal['pathological']==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35f2b784-190f-45ea-9b32-1c6c6bbef641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(447, 442)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_bal_eval = list(sorted(list(eval_patho_bal['index'])+ list(eval_set_normal['index'])))\n",
    "\n",
    "evalbal = final_eval.iloc[ind_bal_eval]\n",
    "\n",
    "len(evalbal[evalbal['pathological']==0]),len(evalbal[evalbal['pathological']==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5e52e358-f70d-4ae0-b3d0-478c2bb8cf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./indices_TUABEx_balanced_train_patho.pkl\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(list(trainbal[trainbal['pathological']==1]['index']), fp)\n",
    "    \n",
    "with open(\"./indices_TUABEx_balanced_train_normal.pkl\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(list(trainbal[trainbal['pathological']==0]['index']), fp)    \n",
    "    \n",
    "with open(\"./indices_TUABEx_balanced_eval_patho.pkl\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(list(evalbal[evalbal['pathological']==1]['index']), fp)\n",
    "    \n",
    "with open(\"./indices_TUABEx_balanced_eval_normal.pkl\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(list(evalbal[evalbal['pathological']==0]['index']), fp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b546daa-9cb1-4bb8-a0d0-c1a4861f7c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./indices_TUABEx_balanced_train_set.pkl\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(list(trainbal['index']), fp)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bf63e46e-2c71-41f3-8c45-a8fd21932585",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./indices_TUABEx_balanced_eval_set.pkl\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(list(evalbal['index']), fp)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "789a994e-4f99-4b42-a4d2-1daebe0682db",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainbal.to_pickle('./TUABEX_train_set_balanced.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ebdeb17-8b2a-432e-9034-069a798ca6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "evalbal.to_pickle('./TUABEX_eval_set_balanced.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed3dc42-7a95-47c5-967c-2f7cde4bf7a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
